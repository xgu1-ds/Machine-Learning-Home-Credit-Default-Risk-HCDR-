{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a name=\"1\"></a>\n",
    "\n",
    "The course project is based on the [Home Credit Default Risk (HCDR)  Kaggle Competition](https://www.kaggle.com/c/home-credit-default-risk/). The goal of this project is to predict whether or not a client will repay a loan. In order to make sure that people who struggle to get loans due to insufficient or non-existent credit histories have a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n",
    "It took place from May 17, 2018 to August 29, 2018. For information on the winning entries, please see [here](https://www.kaggle.com/c/home-credit-default-risk/leaderboard/).\n",
    "\n",
    "In this notebook, we follow a traditional data science project process to address the HCDR challenge of predicting fclients' repayment abilities using telco and transactional information.  \n",
    "\n",
    "* We extract, transform, and load the data (**ETL**).\n",
    "* Along the way, we do some exploratory data analysis (**EDA**).\n",
    "* We conduct **feature engineering**.    \n",
    "* We then **create models, evaluate their performance, and fine tune their parameters**.  \n",
    "* We write up our **results**.  \n",
    "\n",
    "To start you off, we will perform some basic visualization, feature engineering and preprocessing on the data provided.  Then, we build a baseline model using the mean of our training targets to predict our Days in the Hospital for our test set.  Finally, we will build a second model using Logostic regression over the following features to see if we can improve on our baseline model:\n",
    "\n",
    "* numerical features\n",
    "  * \n",
    "* Categorical features\n",
    "  * \n",
    "* Target variable: indicating 0: the loan was repaid or 1: the loan was not repaid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data you plan to use (HDCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a list of datasets. The full dataset will be generated from the tables listed below:**\n",
    "1. [application_test.csv](https://www.kaggle.com/c/home-credit-default-risk/download/application_test.csv): testing application data at Home Credit. \n",
    "2. [application_train.csv](https://www.kaggle.com/c/home-credit-default-risk/download/application_train.csv): training application data at Home Credit with target (0: the loan was repaid or 1: the loan was not repaid).\n",
    "3. [bureau.csv](https://www.kaggle.com/c/home-credit-default-risk/download/bureau.csv): data from other financail institudes.\n",
    "4. [bureau_balance.csv](https://www.kaggle.com/c/home-credit-default-risk/download/bureau_balance.csv): monthly data about the previous credits in bureau.\n",
    "5. [credit_card_balance.csv](https://www.kaggle.com/c/home-credit-default-risk/download/credit_card_balance.csv): previous monthly credit card data.\n",
    "6. [installments_payments.csv](https://www.kaggle.com/c/home-credit-default-risk/download/installments_payments.csv): payment history.\n",
    "7. [POS_CASH_balance.csv](https://www.kaggle.com/c/home-credit-default-risk/download/POS_CASH_balance.csv): monthly data about previous point of sale or cash loans.\n",
    "8. [previous_application.csv](https://www.kaggle.com/c/home-credit-default-risk/download/previous_application.csv): previous applications of clients.\n",
    "\n",
    "    \n",
    "***__Note: subjects to change as the project going on__***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables can be visualized as follows:\n",
    "\n",
    "<img src=\"home_credit_tables_str.png\" width=\"900\" height=\"800\" />\n",
    "***https://www.kaggle.com/c/home-credit-default-risk/data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which machine learning algorithm are you considering to use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data preprocessing and feature engineering, we will consider clustering algorithms (likely via KMeans++) and PCA to reduce dimentionality. Our datasets have a ton of features, some of which are likely to be highly correlated, and consolidating those down will help with efficiency of the final model while still maintaining much of their information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a very logical place to start when dealing with a categorical problem. It's a simple but very powerful algorithm used commonly in practice with excellent results.\n",
    "\n",
    "Other algorithms that will be considered include Naive Bayes, Support Vector machine learning, Stochastic Gradient Decent for optmizing various types of loss, and Decision Trees via random forest and gradient boosting.\n",
    "\n",
    "K-Nearest Neighbor will be considered, but with the sheer number of features present, could potentially be too computationally expensive to be worth investing in, considering its relative simplicity. Experimentation with subsets will bear all this out.\n",
    "\n",
    "All algorithms will be tuned and optimized with a variety of hyperparamters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics that you might use to measure success (standard metrics and domain specific metrics)\n",
    "Metric: ROC AUC\n",
    " is a common classification metric known as the Receiver Operating Characteristic Area Under the Curve (ROC AUC, also sometimes called AUROC).\n",
    "\n",
    "The ROC AUC may sound intimidating, but it is relatively straightforward once you can get your head around the two individual concepts. The Reciever Operating Characteristic (ROC) curve graphs the true positive rate versus the false positive rate:\n",
    "\n",
    "\n",
    "<img src=\"ROC.png\" width=\"600\" height=\"600\" />\n",
    "\n",
    "A single line on the graph indicates the curve for a single model, and movement along a line indicates changing the threshold used for classifying a positive instance. The threshold starts at 0 in the upper right to and goes to 1 in the lower left. A curve that is to the left and above another curve indicates a better model. For example, the blue model is better than the red model, which is better than the black diagonal line which indicates a naive random guessing model.\n",
    "\n",
    "The Area Under the Curve (AUC) explains itself by its name! It is simply the area under the ROC curve. (This is the integral of the curve.) This metric is between 0 and 1 with a better model scoring higher. A model that simply guesses at random will have an ROC AUC of 0.5.\n",
    "\n",
    "When we measure a classifier according to the ROC AUC, we do not generation 0 or 1 predictions, but rather a probability between 0 and 1. This may be confusing because we usually like to think in terms of accuracy, but when we get into problems with inbalanced classes (we will see this is the case), accuracy is not the best metric. For example, if I wanted to build a model that could detect terrorists with 99.9999% accuracy, I would simply make a model that predicted every single person was not a terrorist. Clearly, this would not be effective (the recall would be zero) and we use more advanced metrics such as ROC AUC or the F1 score to more accurately reflect the performance of a classifier. A model with a high ROC AUC will also have a high accuracy, but the ROC AUC is a better representation of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A block diagram (Gantt diagram) of the key steps involved in completing this task and timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/shared/Documents/s526/I526_AML_SP19/class work/S526_project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the pipeline steps you plan to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical pipeline on numerical features and categorical pipeline on categorical features. The numerical pipeline has DataFrameSelector, SimpleImputer with mean and StandardScaler transformations. The DataframeSelector is to select specific attributes from the DataFrame. The SimpleImputer is completing missing values with attribute mean. The StandardScaler is standardized features by removing the mean and scaling to unit variance. The categorical pipeline has DataFrameSelector, SimpleImputer with constant and OneHotEncoder transformations. The SimpleImputer is completing missing values with constant strategy. The OneHotEncoder transformation is used to encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings and creates a binary column for each category and returns a sparse matrix or dense array. The full pipeline is generated by combining numerical and categorical pipelines. The full pipeline is used on training data to transform it for model input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List team members "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indication as to which part of the project each team member will be working on. e.g., I plan to build text-based features based on the product description and optimize random forest models using all available features and work on precision-recall measures; Naveen might sign up for processing all the numerical data (standardizing, dealing with missing values) and optimize regularized logistic regression models using all available features, and work on F-measure metrics. As a team you will need to integrate your SKlearn pipelines, your results,  your discussion, and conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major updates in the introductory phase.\n",
    "* Downloaded the data\n",
    "* ETL the data to generate training and test sets\n",
    "* Some basic feature engineering\n",
    "* Built a pipeline for feature engineering and for modeling\n",
    "* Built a baseline model using linear regression\n",
    "* Started planning for phase 2\n",
    "\n",
    "### Major plans for Phase 1\n",
    "* Plan to engineering features\n",
    "* Finetune hyperparameters for various machine learning pipelines\n",
    "* Find and complete the \"TASKS\" throughout the notebook; use these as starting points for your own experimentation\n",
    "\n",
    "### Open problems\n",
    " * None to report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Phase-1 Updates:\n",
    "\n",
    "### Major updates in Phase 1 \n",
    "* USE THIS SECTION AS A FRAMEWORK FOR UPDATING YOUR PROGRESS AFTER EACH PHASE\n",
    "\n",
    "### Major plans for the next Phase \n",
    "* .....\n",
    "\n",
    "### Open problems\n",
    " * None to report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Phase-2 Updates:\n",
    "\n",
    "### Major updates in Phase 2 \n",
    "* C\n",
    "\n",
    "### Major plans for the next Phase \n",
    "* .....\n",
    "\n",
    "### Open problems\n",
    " * None to report\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "## Final Project Submission:\n",
    "\n",
    "### Major past updates for the final phase \n",
    "* ....\n",
    "\n",
    "### Plans for future improvement \n",
    "* .....\n",
    "\n",
    "### Open problems\n",
    " * None to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
